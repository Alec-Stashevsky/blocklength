---
title: "Tuning Block-Length Selection Methods"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{selecting-blocklengths}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: references.bib
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  out.width = "50%",
  dev = "svg"
)
```


This vignette builds a framework for tuning the block-selection algorithms in `{blocklength}.` Both the @10.1093/biomet/82.3.561 ("HHJ") and @doi:10.1081/ETC-120028836 ("PWSD") methods require the user to select somewhat arbitrary parameters which may affect the reliability of the block-length optimization. We will go through examples that present *problematic* output and understand how to diagnose and remedy these situations under both the HHJ and PWSD frameworks.

## Tuning Parameters

This section will discuss the tuning parameters of each optimization method. Though some parameters are set to a default, often times manual inspection and adjustments will need to be made to identify and remedy problematic solutions such as local optima or corner solutions. 

An overview of the tuning parameters: 

- `hhj()`
  - `sub_sample`
  - `pilot_block_length`
  
- `pwsd()`
  - `K_N`
  - `M_max`
  - `b_max`
  - `c`
  

### HHJ

The accuracy of `hhj()` depends on the choice of two tuning parameters. The first is the size of the subsample series to perform the cross-validation over. This is the argument `sub_sample =`, or the parameter $m$ in @10.1093/biomet/82.3.561. At this stage $m$ is unknown, but there are some restrictions $m$ must follow.

Since `hhj()` must iterate over multiple subsamples it must be that $m < n$ where $n$ is the length of the series provided in argument `series =`, *i.e.* `length(series)`. More specifically, it must be that

$$ m^{-1} + n^{-1}m  = o(1) \text{ as } n \rightarrow \infty \ . $$

As a default, `sub_sample = NULL` which sets $m = n^{1 / 5} * n^{1 / k}$ satisfying the above restrictions. Users can override this by directly setting $m$ as some numeric constant supplied to `subsample = .` $m$ should be an integer, but `hhj()` will round this to a whole number automatically.

Though $m$ is almost entirely arbitrary, the second tuning parameter for `hhj()` is less so. This is the `pilot_block_length =` argument, or the parameter $l$ in @10.1093/biomet/82.3.561 also denoted as $l^{*}_n$ in the @lahiri2003resampling treatment. The `pilot_block_length` is the block-length used to perform the initial block-bootstrap of `series = .`

To reduce the effect of the choice of $l$ on the accuracy of the optimization, `hhj()` replaces $l$ with the previous iteration's estimate of the optimal block-length $l^*$ (or $\hat{l^0_n}$ in @lahiri2003resampling), repeating until convergence or the iteration limit implied by `n_iter = ` is reached. The number supplied to `pilot_block_length = ` is only used on the first iteration of `hhj()` hence it is referred to as a *pilot* parameter.

In practice, `hhj()` estimates $l^*$ by minimizing an $MSE$ function for each possible block-length for a block-bootstrap of a subsample from `series` with length $m.$ That is, `length(series) = sub_sample.` The block-length that minimizes the $MSE$ on a given subsample then used to estimate $l^*$ which replaces `pilot_block_length` for the next iteration. However, because we minimize the $MSE$ over a subsample of length $m$, we must first scale this block-length back to the original series of length $n.$ This is accomplished by employing a sequence acceleration technique from numerical analysis called the *Richardson Extrapolation.* This technique was first developed in 1910, *see* @doi:10.1098/rsta.1911.0009.


## PWSD

Like `hhj()`, the accuracy of `pwsd()` is also subject to a somewhat arbitrary choice of tuning parameters. Unlike the HHJ method, however, the tuning parameters for `pwsd()` deal exclusively with the way the method adapts to the underlying correlation structure of the series, rather than setting lengths and sizes from which to cross-validate subsamples.

@doi:10.1081/ETC-120028836 discuss their proposed defaults and some techniques to adjust tuning parameters largely in *footnote c* on page 59. The first tuning parameter of `pwsd()` is `K_N = `, or $K_N$ in @doi:10.1081/ETC-120028836. `K_N` takes some integer value that indicates the maximum number of lags for which to apply the implied hypothesis test over the correlogram of the series. That is, `K_N` is the number of consecutive lags in the correlogram that must appear *insignificant* in order to select the optimal bandwidth $M$ for the flat-top lag window. @doi:10.1081/ETC-120028836 suggest letting $M = 2\hat{m}$ where $\hat{m}$ is the smallest positive integer such that the following `K_N` consecutive lags appear insignificant in respect so some implied level of significance, discussed later. This concept is admittedly a bit confusing and will be discussed with more detail in following sections. By default, `K_N  = NULL` which sets $K_N = max(5, \lceil \ \log_{10}n\rceil \ )$ per the suggestion of @doi:10.1081/ETC-120028836. Note $K_N$ must be some non-decreasing integer valued function of $N$ such that $K_N = o(log_{10}N)$ where $N$ is again the length of the series supplied to `data = `, *i.e.* `nrow(data)`$= N.$

The next tuning parameter `M_max =` (denoted as $M_{max}$ below) acts as an upper-bound for $M$ such that

$$2 * \hat{m} > M_{max} \Rightarrow M = M_{max} \ .$$

Thus, `M_max` allows the user to expand or contract the maximum size of the bandwidth for the flat-top lag windows of @https://doi.org/10.1111/j.1467-9892.1995.tb00223.x. If it is the case that $2 * \hat{m} < M_{max}$, then *only* changing the value of `M_max` will not have an affect on the method's accuracy.

We can inspect the output of `pwsd()` to assess any interim values that may help us understand how to further tune the calculation. For example, we can find value estimated as $\hat{m}$ by inspecting the `$parameters` component of `pwsd()` output objects, *i.e.* objects where `class = "pwsd".`

To show this lets first do some housekeeping and attach `blocklength.`
```{r setup}
library(blocklength)
set.seed(993)
```


Now, we generate a stationary $AR(1)$ time series and assess and optimal block-length using the PWSD method:
```{r Inspect Parameters}
# Generate an AR(1) simulation
sim <- stats::arima.sim(list(order = c(1, 0, 0), ar = 0.5),
                        n = 500, innov = rnorm(500))

# Run the PWSD method
b <- pwsd(sim, correlogram = FALSE)

# Inspect interim parameters
b$parameters
```


The next tuning parameter is `b_max = ` which acts as an upper-bound for the optimal block-length. If `pwsd()` estimates some value for the optimal block-length such that `b_Stationary > b_max` or `b_Circular > b_max` then the optimal output will be set to `b_max.` By default, `b_max = NULL` which sets `b_max = ceiling(min(3 * sqrt(n), n / 3)).` We can inspect the output of `pwsd()` to see if the selected block-lengths are being censored by `b_max.`

```{r Inpect b_max}
# Test if optimal block-length are censored by b_max
b$parameters[, "b_max"] > b$BlockLength
```

Here we can see our optimal block-lengths is between 8 and 9 observations. These are far below the default maximum value.

The final tuning parameter for `pwsd()` is the argument `c = ` also denoted as $c$ in @doi:10.1081/ETC-120028836. At a high-level $c$ acts as a pseudo confidence-level to conduct the implied hypothesis tests on the series' correlogram. Thus, it must be that $c > 0.$ If the auto-correlation of a given lag $k$ is above some critical value $\rho_{critical}$, then that lag is considered *significant.* This critical value is defined as 

$$\rho_{critical} = c\sqrt{\log_{10}N / N}.$$

@doi:10.1081/ETC-120028836 suggest a value of $c = 2$ but by default, `pwsd()` treats $c$ as a standard two-tailed 95% confidence interval by setting `c = qnorm(0.975).` This is quite close the suggested value of 2. Using the correlogram output, either by setting `correlogram = TRUE` or using the corresponding `plot` method, we can visualize $\rho_{critical}$ as the dashed magenta lines that appear on the plot.

```{r Correlogram, fig.show="hold", out.width="40%"}
plot(b, main = "c = qnorm(0.975)")

# Expand confidence level and plot again
b2 <- pwsd(sim, c = 4, correlogram = FALSE)
plot(b2, main = "c = 4")
```


## Diagnosing Non-Optimal Solutions

## References:
